####################################################
## script to perform logistic regression 
## using Gibbs sampling, adapted from probit_g and 
## probit_gd matlab scripts from James P LeSage
## www.spatial-econometrics.com 
##
## Mark Patrick Roeling, dec 2017       
## Uni of Oxford, mark.roeling@stats.ox.ac.uk     
####################################################

library(truncnorm)
set.seed(2017)
rm(list = ls(all = TRUE))
setwd("/Users/mark/Desktop/")
source('/Users/mark/Documents/networks/scripts/gibbs_zsamplingfunctions.R')

# one function called stdn_cdf is deteriorated in Matlab, redefined the function as 'stdn_cdf' in R using the original:
# https://nl.mathworks.com/matlabcentral/fileexchange/54382-bayesian-spatial-propensity-score-matching--bs-psm-?focused=5777488&tab=function

n = 100
k = 3

evec = c(-2.1649,1.0482,-0.2421,-1.5811,0.7016,0.3623,-0.5465,-1.1631,-1.5832,-1.2530,-2.8292,0.7071,-0.4503,
         -0.6932,0.2406,-0.7714,1.1690,-1.3613,-0.7241,0.3363,-1.1239,-0.7746,-0.6175,-0.7341,-0.3203,1.0155,-0.1068,
         -1.3442,0.7001,0.3468,-2.3689,-1.1289,0.5530,-0.1856,1.2539,0.2468,-1.7117,0.4979,-0.5146,2.1424,1.6750,-0.8922,
         -1.0076,-1.1326,0.1120,0.6513,-0.1321,0.1912,1.1180,-1.0853,0.8197,-0.7159,-0.5914,0.0108,-0.7637,-0.6921,-0.5076,
         1.1735,0.2909,-1.4150,-0.3688,0.8434,-0.5068,1.1835,-0.8096,-1.2107,2.7214,0.9596,0.9823,-0.9796,0.0584,
         0.7852,-1.1927,-0.2304,-1.2754,0.5146,-0.1397,-1.4505,-0.6056,-2.7933,1.6350,-0.9349,-1.3720,0.8084,1.1821,
         3.0683,-1.4274,-0.8534,0.4874,0.8812,-0.6914,-0.5893,-0.4662,0.6118,0.8971,0.5314,0.9357,1.3341,-1.6443,-0.0699)

#evec = rnorm(n)
tt = 1:n

x = t(matrix(c(1.0000,-1.1992,0.8892,
               1.0000,-0.4597,-0.0068,
               1.0000,-0.6083,-1.2766,
               1.0000,-0.4497,0.8584,
               1.0000,0.3347,-1.8136,
               1.0000,-0.7363,0.6044,
               1.0000,0.1718,-0.3849,
               1.0000,-1.6562,-0.1793,
               1.0000,1.0753,0.8210,
               1.0000,0.1790,0.0104,
               1.0000,-0.4180,0.0793,
               1.0000,-0.4632,-1.0888,
               1.0000,1.0599,0.3974,
               1.0000,-0.4408,0.2126,
               1.0000,1.3138,-0.6178,
               1.0000,-0.9104,1.2342,
               1.0000,-2.0709,-0.6280,
               1.0000,-0.2507,-1.0847,
               1.0000,1.0850,0.7804,
               1.0000,-0.0502,0.3008,
               1.0000,2.0299,1.0907,
               1.0000,1.0767,-0.5607,
               1.0000,-0.8342,-1.0162,
               1.0000,-1.4967,-1.3116,
               1.0000,-0.4002,-0.7383,
               1.0000,1.6026,-1.4370,
               1.0000,1.0367,0.3559,
               1.0000,-0.5502,-0.4701,
               1.0000,-0.9232,0.5452,
               1.0000,0.2555,0.1943,
               1.0000,0.6748,-0.7283,
               1.0000,0.5326,-0.0274,
               1.0000,-1.0021,2.0603,
               1.0000,-1.4109,0.1242,
               1.0000,1.0849,-1.8766,
               1.0000,0.0554,-0.1819,
               1.0000,-0.6049,0.2138,
               1.0000,1.0476,0.3299,
               1.0000,0.4131,1.1243,
               1.0000,0.7288,1.1319,
               1.0000,1.0370,-0.4814,
               1.0000,0.0647,0.1535,
               1.0000,-0.8706,-1.5366,
               1.0000,2.6344,0.2234,
               1.0000,0.8868,0.4913,
               1.0000,-0.2727,-1.1270,
               1.0000,-0.5280,0.9806,
               1.0000,-0.8539,-1.7979,
               1.0000,0.1988,1.7304,
               1.0000,0.8460,1.2689,
               1.0000,0.0805,-0.9585,
               1.0000,-1.1721,0.7283,
               1.0000,0.1249,0.5242,
               1.0000,1.1886,0.2256,
               1.0000,0.7258,-0.2631,
               1.0000,1.4456,1.0930,
               1.0000,-1.2827,-0.4891,
               1.0000,-0.0987,0.8305,
               1.0000,1.3339,0.5182,
               1.0000,1.3096,0.4238,
               1.0000,-0.2225,1.0098,
               1.0000,-0.8259,0.3020,
               1.0000,0.0796,-1.3410,
               1.0000,0.4196,1.6861,
               1.0000,0.0190,0.0605,
               1.0000,-0.7970,0.6258,
               1.0000,-0.5927,-0.3952,
               1.0000,1.0104,0.8735,
               1.0000,-0.5659,-1.1166,
               1.0000,-1.1992,-0.8046,
               1.0000,0.6437,-1.0711,
               1.0000,-0.1002,-0.1896,
               1.0000,0.4762,-2.1128,
               1.0000,0.8147,0.1114,
               1.0000,0.3634,-0.0106,
               1.0000,0.5113,-0.0294,
               1.0000,1.6252,1.0927,
               1.0000,1.3590,-1.1020,
               1.0000,0.3984,-1.5050,
               1.0000,-0.7858,1.1545,
               1.0000,-0.2964,1.3235,
               1.0000,-0.6874,-1.2227,
               1.0000,-1.2464,-1.6426,
               1.0000,-1.1035,-2.5110,
               1.0000,-0.1050,0.1483,
               1.0000,-0.3308,-0.0263,
               1.0000,-0.5222,-0.3563,
               1.0000,-1.3793,-1.0741,
               1.0000,-0.3458,-1.8788,
               1.0000,-1.6572,0.3533,
               1.0000,-0.3874,0.4614,
               1.0000,0.1047,0.6151,
               1.0000,1.0301,-0.0951,
               1.0000,0.8610,0.7532,
               1.0000,-2.3923,-0.5016,
               1.0000,-0.1585,-2.3857,
               1.0000,2.9003,-0.4364,
               1.0000,0.0299,0.3144,
               1.0000,-0.9912,-0.0287,
               1.0000,-1.1168,0.2282), k, n))

#x = cbind(rnorm(n), rnorm(n), rnorm(n))
#x[1:n,1] = rep(1, n)

b = rep(1,k)
b[3] = -2.0;
b[2] = 2.0;
b[1] = -0.5;

y = x%*%b + 0.2*evec  # predict dependent Y

yc = rep(0,n)         # now censor the data
for(i in 1:n){
  if(y[i,1] > 0){
    yc[i] = 1
  }else{
    yc[i] = 0
  }
}

sum(yc)               # how much persons have a 1?

Vnames = as.character(c('y','constant','x1','x2'))

prior = 40 ; names(prior) = "rval"      # heteroscedastic prior
ndraw = 10100
nomit = 100

# example full prior
prior <- NULL
prior$rval = 40 
#prior$m = 0
#prior$k = 0
#prior$beta = 0
#prior$bcov = 1e+12
#prior$nu = 0
#prior$do = 0

#model = glm(yc ~ x[,2] + x[,3], family = "binomial")


result = probit_g(yc,x,ndraw,nomit,prior)
colMeans(result$bdraw)

### imputation script ###
result = probit_g_mp(yc,x,ndraw,nomit,prior)
colMeans(result$bdraw)

#imputation = read.table("/Users/mark/Documents/Gibbs_imputation.txt")
imputation = as.data.frame(result$imputation)
colnames(imputation) = c("XtB", "ydrawtrue", "ydrawfalse", "xstarbetahat", "errorimptrue", "errorimpfalse")
write.table(imputation, "/Users/mark/Documents/Gibbs_imputation.txt", sep = "\t", quote = F, col.names = T, row.names = F)

